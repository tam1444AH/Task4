{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4f2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, List, Dict\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517b7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Coordinate = Tuple[int, int]\n",
    "\n",
    "@dataclass\n",
    "class PDWorldEnvironment:\n",
    "    grid_size: int = 5\n",
    "    pickup_locations: Dict[Coordinate, int] = field(default_factory=lambda: {(3,5): 10, (4,10): 10})\n",
    "    dropoff_locations_capacity: Dict[Coordinate, int] = field(default_factory=lambda: {(1,1): 5, (2,5): 5, (3,3): 5, (5,5): 5})\n",
    "    \n",
    "    female_agent_start: Coordinate = (1, 3)\n",
    "    male_agent_start: Coordinate = (5, 3)\n",
    "    \n",
    "    \n",
    "    # These are the reward parameters\n",
    "    step_cost: float = -1.0\n",
    "    pickup_reward: float = 1.0\n",
    "    dropoff_reward: float = 10.0\n",
    "    invalid_penalty: float = -5.0\n",
    "    terminal_bonus: float = 20.0\n",
    "    \n",
    "    \n",
    "    use_counts_in_state: bool = False\n",
    "    \n",
    "    \n",
    "    # this is the internal state\n",
    "    f_agent_pos: Coordinate = field(init=False)\n",
    "    m_agent_pos: Coordinate = field(init=False)\n",
    "    carryingF: int = field(init=False, default=0)\n",
    "    carryingM: int = field(init=False, default=0)\n",
    "    pickups: Dict[Coordinate, int] = field(init=False)\n",
    "    dropoffs: Dict[Coordinate, int] = field(init=False)\n",
    "    turn: str = field(init=False) # Its either 'F' or 'M' turn\n",
    "    episode_steps: int = field(init=False, default=0)\n",
    "    \n",
    "    \n",
    "    actions = ['N', 'S', 'E', 'W', 'P', 'D']  # North, South, East, West, Pickup, Dropoff\n",
    "    \n",
    "    def reset(self):\n",
    "        self.f_agent_pos = self.female_agent_start\n",
    "        self.m_agent_pos = self.male_agent_start\n",
    "        self.carryingF = 0\n",
    "        self.carryingM = 0\n",
    "        self.pickups = dict(self.pickup_locations)\n",
    "        self.dropoffs = dict(self.dropoff_locations_capacity)\n",
    "        self.turn = 'F'\n",
    "        self.episode_steps = 0\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def is_terminal(self) -> bool:\n",
    "        return all(self.dropoffs[pos] >= cap for pos, cap in self.dropoff_locations_capacity.items())\n",
    "    \n",
    "    def total_remaining(self) -> int:\n",
    "        total_capacity = sum(self.dropoff_locations_capacity.values())\n",
    "        total_dropped = sum(self.dropoffs.values())\n",
    "        return max(0, total_capacity - total_dropped)\n",
    "    \n",
    "    def bucket_remaining(self) -> int: # this function returns how many buckets are remaining to be dropped off\n",
    "        remaining = self.total_remaining()\n",
    "        return min(4, (remaining + 4) // 5)\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.use_counts_in_state:\n",
    "            pick = tuple(sorted(self.pickups.items()))\n",
    "            drop = tuple(sorted(self.dropoffs.items()))\n",
    "        else:\n",
    "            pick = ()\n",
    "            drop = (self.bucket_remaining(),)\n",
    "            \n",
    "        return (self.f_agent_pos, self.m_agent_pos, self.carryingF, self.carryingM, pick, drop, self.turn)\n",
    "    \n",
    "    def other_agent(self, who: str) -> str:\n",
    "        return 'M' if who == 'F' else 'F'\n",
    "    \n",
    "    def move(self, pos: Coordinate, action: str) -> Coordinate:\n",
    "        r, c = pos\n",
    "        \n",
    "        if action == 'N' and r > 1: r -= 1\n",
    "        elif action == 'S' and r < self.grid_size: r += 1\n",
    "        elif action == 'E' and c < self.grid_size: c += 1\n",
    "        elif action == 'W' and c > 1: c -= 1\n",
    "        \n",
    "        return (r, c)\n",
    "    \n",
    "    def step(self, action: str):\n",
    "        who = self.turn\n",
    "        other = self.other_agent(who)\n",
    "        reward = self.step_cost\n",
    "        done = False\n",
    "        invalid = False\n",
    "        \n",
    "        agent_pos = self.f_agent_pos if who == 'F' else self.m_agent_pos # get current agent position\n",
    "        other_pos = self.m_agent_pos if who == 'F' else self.f_agent_pos # get other agent position\n",
    "        carrying = self.carryingF if who == 'F' else self.carryingM\n",
    "        \n",
    "        # here we perform the action\n",
    "        if action in ['N', 'S', 'E', 'W']:\n",
    "            new_pos = self.move(agent_pos, action)\n",
    "            if new_pos == other_pos:\n",
    "                invalid = True\n",
    "            else:\n",
    "                agent_pos = new_pos\n",
    "        elif action == 'P':\n",
    "            if agent_pos in self.pickups and self.pickups[agent_pos] > 0 and carrying == 0:\n",
    "                self.pickups[agent_pos] -= 1\n",
    "                carrying = 1\n",
    "                reward += self.pickup_reward\n",
    "            else:\n",
    "                invalid = True\n",
    "        elif action == 'D':\n",
    "            if agent_pos in self.dropoff_locations_capacity and carrying == 1:\n",
    "                if self.dropoffs[agent_pos] < self.dropoff_locations_capacity[agent_pos]:\n",
    "                    self.dropoffs[agent_pos] += 1\n",
    "                    carrying = 0\n",
    "                    reward += self.dropoff_reward\n",
    "                else:\n",
    "                    invalid = True\n",
    "            else:\n",
    "                invalid = True\n",
    "        else:\n",
    "            invalid = True\n",
    "            \n",
    "        if invalid:\n",
    "            reward += self.invalid_penalty\n",
    "        if who == 'F':\n",
    "            self.f_agent_pos = agent_pos\n",
    "            self.carryingF = carrying\n",
    "        else:\n",
    "            self.m_agent_pos = agent_pos\n",
    "            self.carryingM = carrying\n",
    "            \n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        if self.is_terminal():\n",
    "            reward += self.terminal_bonus\n",
    "            done = True\n",
    "            \n",
    "        self.turn = other\n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    \n",
    "    def applicable_ops(self, who: str) -> List[str]: # this function returns list of applicable operations for the given agent\n",
    "        agent_pos = self.f_agent_pos if who == 'F' else self.m_agent_pos\n",
    "        carrying = self.carryingF if who == 'F' else self.carryingM\n",
    "        \n",
    "        ops = []\n",
    "        if agent_pos in self.pickups and self.pickups[agent_pos] > 0 and carrying == 0:\n",
    "            ops.append('P')\n",
    "        if agent_pos in self.dropoff_locations_capacity and carrying == 1 and self.dropoffs[agent_pos] < self.dropoff_locations_capacity[agent_pos]:\n",
    "            ops.append('D')\n",
    "        \n",
    "        if not ops:\n",
    "            ops = ['N', 'S', 'E', 'W']\n",
    "            \n",
    "        return ops        \n",
    "    \n",
    "    def manhattan_distance(self) -> int:\n",
    "        r1, c1 = self.f_agent_pos\n",
    "        r2, c2 = self.m_agent_pos\n",
    "        return abs(r1 - r2) + abs(c1 - c2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self, actions: List[str]):\n",
    "        self.actions = actions\n",
    "        self.Q = defaultdict(lambda: np.zeros(len(actions), dtype=float))\n",
    "        \n",
    "    def best_action(self, state) -> int:\n",
    "        q = self.Q[state]\n",
    "        max_val = np.max(q)\n",
    "        idxs = np.flatnonzero(q == max_val)\n",
    "        return random.choice(idxs)\n",
    "    \n",
    "    def update_q(self, state, action_idx, target, alpha):\n",
    "        q = self.Q[state]\n",
    "        q[action_idx] += (1 - alpha) * q[action_idx] + alpha * target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentConfig: # configuration for the agent\n",
    "    alpha: float\n",
    "    gamma: float\n",
    "    policy: str # This would be 'PRANDOM', 'PGREEDY', or 'PEXploit'\n",
    "    epsilon: float = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce89309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentLearner: # We choose an action for our agent based on a policy\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class PDWorld: # We initialize the PD-World environment\n",
    "    def __init__(self, grid_size=5, pickup_locs=[(3,5), (4,2)], dropoff_locs=[(1,1), (1,5), (3,3), (5,5)], agent_f_pos=(1,3), agent_m_pos=(5,3)):\n",
    "        self.grid_size = grid_size\n",
    "        self.pickup_locs = pickup_locs\n",
    "        self.dropoff_locs = dropoff_locs\n",
    "        \n",
    "        self.pickup_blocks = {loc: 10 for loc in pickup_locs} # Each pickup location starts with 10 blocks\n",
    "        \n",
    "        self.dropoff_blocks = {loc: 0 for loc in dropoff_locs} \n",
    "        \n",
    "        self.agent_f_pos = agent_f_pos\n",
    "        self.agent_m_pos = agent_m_pos\n",
    "        \n",
    "        self.agent_f_holding = False\n",
    "        self.agent_m_holding = False\n",
    "        \n",
    "        self.initial_pickup_locs = copy.deepcopy(pickup_locs)\n",
    "        self.initial_agent_f_pos = agent_f_pos\n",
    "        self.initial_agent_m_pos = agent_m_pos\n",
    "        \n",
    "    def reset(self, keep_qtable=True): # We reset the world but keep q-tables\n",
    "        self.pickup_blocks = {loc: 10 for loc in self.pickup_locs}\n",
    "        self.dropoff_blocks = {loc: 0 for loc in self.dropoff_locs}\n",
    "        self.agent_f_pos = self.initial_agent_f_pos\n",
    "        self.agent_m_pos = self.initial_agent_m_pos\n",
    "        self.agent_f_holding = False\n",
    "        self.agent_m_holding = False\n",
    "        \n",
    "    def is_terminal(self): # In this function we check if terminal state is reached, so all drop-off locations have 5 blocks\n",
    "        return all(blocks == 5 for blocks in self.dropoff_blocks.values())\n",
    "    \n",
    "    def get_state(self, agent='F'): # This is where we return the current state for a given agent\n",
    "        if agent == 'F':\n",
    "            return (self.agent_f_pos, self.agent_f_holding, self.agent_m_pos)\n",
    "        else:\n",
    "            return (self.agent_m_pos, self.agent_m_holding, self.agent_f_pos)\n",
    "    \n",
    "    def get_valid_actions(self, agent='F'): # This function returns the valid actions for a given agent\n",
    "\n",
    "        actions = []\n",
    "        pos = self.agent_f_pos if agent == 'F' else self.agent_m_pos\n",
    "        other_pos = self.agent_m_pos if agent == 'F' else self.agent_f_pos\n",
    "        holding = self.agent_f_holding if agent == 'F' else self.agent_m_holding\n",
    "        \n",
    "\n",
    "        if pos[0] > 1 and (pos[0]-1, pos[1]) != other_pos:\n",
    "            actions.append('N')\n",
    "        if pos[0] < self.grid_size and (pos[0]+1, pos[1]) != other_pos:\n",
    "            actions.append('S')\n",
    "        if pos[1] < self.grid_size and (pos[0], pos[1]+1) != other_pos:\n",
    "            actions.append('E')\n",
    "        if pos[1] > 1 and (pos[0], pos[1]-1) != other_pos:\n",
    "            actions.append('W')\n",
    "        \n",
    "        if not holding and pos in self.pickup_locs and self.pickup_blocks.get(pos, 0) > 0:\n",
    "            actions.append('P')\n",
    "        \n",
    "        if holding and pos in self.dropoff_locs and self.dropoff_blocks.get(pos, 0) < 5:\n",
    "            actions.append('D')\n",
    "            \n",
    "        return actions\n",
    "    \n",
    "    def execute_action(self, agent, action): # We execute a selected action\n",
    "        if agent == 'F':\n",
    "            pos = self.agent_f_pos\n",
    "            holding = self.agent_f_holding\n",
    "        else:\n",
    "            pos = self.agent_m_pos\n",
    "            holding = self.agent_m_holding\n",
    "            \n",
    "        reward = -1\n",
    "        \n",
    "        if action == 'N':\n",
    "            new_pos = (pos[0]-1, pos[1])\n",
    "        elif action == 'S':\n",
    "            new_pos = (pos[0]+1, pos[1])\n",
    "        elif action == 'E':\n",
    "            new_pos = (pos[0], pos[1]+1)\n",
    "        elif action == 'W':\n",
    "            new_pos = (pos[0], pos[1]-1)\n",
    "        elif action == 'P':\n",
    "            if pos in self.pickup_locs and self.pickup_blocks[pos] > 0:\n",
    "                self.pickup_blocks[pos] -= 1\n",
    "                if agent == 'F':\n",
    "                    self.agent_f_holding = True\n",
    "                else:\n",
    "                    self.agent_m_holding = True\n",
    "                reward = 13\n",
    "            new_pos = pos\n",
    "        elif action == 'D':\n",
    "            if pos in self.dropoff_locs and self.dropoff_blocks[pos] < 5:\n",
    "                self.dropoff_blocks[pos] += 1\n",
    "                if agent == 'F':\n",
    "                    self.agent_f_holding = False\n",
    "                else:\n",
    "                    self.agent_m_holding = False\n",
    "                reward = 13\n",
    "            new_pos = pos\n",
    "        else:\n",
    "            new_pos = pos\n",
    "            \n",
    "        if agent == 'F':\n",
    "            self.agent_f_pos = new_pos\n",
    "        else:\n",
    "            self.agent_m_pos = new_pos\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def visualize(self): # This function visualizes the current state of our world\n",
    "        grid = np.zeros((self.grid_size, self.grid_size))\n",
    "        \n",
    "        for loc in self.pickup_locs:\n",
    "            if self.pickup_blocks.get(loc, 0) > 0:\n",
    "                grid[loc[0]-1, loc[1]-1] = 1\n",
    "        \n",
    "        \n",
    "        for loc in self.dropoff_locs:\n",
    "            grid[loc[0]-1, loc[1]-1] = 2\n",
    "        \n",
    "        grid[self.agent_f_pos[0]-1, self.agent_f_pos[1]-1] = 3\n",
    "        grid[self.agent_m_pos[0]-1, self.agent_m_pos[1]-1] = 4\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(grid, cmap='Set3')\n",
    "        plt.title('PD-World State')\n",
    "        plt.colorbar(ticks=[0, 1, 2, 3, 4], label='0:Empty, 1:Pickup, 2:Dropoff, 3:AgentF, 4:AgentM')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "class QLearningAgent: # This class implements our Q-Learning/Sarsa agent\n",
    "    def __init__(self, agent_name, alpha=0.3, gamma=0.5, algorithm='Q-learning'):\n",
    "        self.agent_name = agent_name\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.algorithm = algorithm\n",
    "        self.q_table = defaultdict(lambda: defaultdict(float)) # This is a nested dictionary for Q-values: state(which is a tuple) -> action -> Q-value\n",
    "        \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table[state][action]\n",
    "    \n",
    "    def choose_action(self, state, valid_actions, policy='PRANDOM'): # This function allows us to choose an action based on the selected policy\n",
    "\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "            \n",
    "\n",
    "        if 'P' in valid_actions or 'D' in valid_actions:\n",
    "            if 'P' in valid_actions:\n",
    "                return 'P'\n",
    "            if 'D' in valid_actions:\n",
    "                return 'D'\n",
    "        \n",
    "        if policy == 'PRANDOM':\n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        elif policy == 'PGREEDY':\n",
    "            q_values = {action: self.get_q_value(state, action) for action in valid_actions}\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "        \n",
    "        elif policy == 'PEXPLOIT':\n",
    "            if random.random() < 0.8:\n",
    "                q_values = {action: self.get_q_value(state, action) for action in valid_actions}\n",
    "                max_q = max(q_values.values())\n",
    "                best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "                return random.choice(best_actions)\n",
    "            else:\n",
    "                return random.choice(valid_actions)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, next_action, valid_next_actions): # This function updates the Q-values based on the selected algorithm\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        \n",
    "        if self.algorithm == 'Q-learning':\n",
    "            if valid_next_actions:\n",
    "                max_next_q = max([self.get_q_value(next_state, a) for a in valid_next_actions])\n",
    "            else:\n",
    "                max_next_q = 0\n",
    "            new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        \n",
    "        else: # SARSA algorithm\n",
    "            if next_action:\n",
    "                next_q = self.get_q_value(next_state, next_action)\n",
    "            else:\n",
    "                next_q = 0\n",
    "            new_q = current_q + self.alpha * (reward + self.gamma * next_q - current_q)\n",
    "        \n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "def run_experiment(world, agent_f, agent_m, total_steps, policy_schedule, experiment_name, pickup_change_config=None, seed=42): # This is the function we use to run our experiments\n",
    "    set_seed(seed)\n",
    "    # An episode consists of both agents taking actions until a terminal state is reached\n",
    "    rewards_history = []\n",
    "    steps_per_episode = []\n",
    "    total_rewards_per_episode = []\n",
    "    terminal_count = 0\n",
    "    current_step = 0\n",
    "    episode_steps = 0\n",
    "    episode_reward = 0\n",
    "    \n",
    "    current_policy = policy_schedule[0][1] # Initialize the current policy\n",
    "    policy_idx = 0\n",
    "    \n",
    "    print(f\"Running {experiment_name} (Seed: {seed})\")\n",
    "    \n",
    "    while current_step < total_steps:\n",
    "        if world.is_terminal():\n",
    "            terminal_count += 1\n",
    "            steps_per_episode.append(episode_steps)\n",
    "            total_rewards_per_episode.append(episode_reward)\n",
    "            \n",
    "            print(f\"Terminal state {terminal_count} reached at step {current_step}\")\n",
    "            print(f\"Episode steps: {episode_steps}, Episode reward: {episode_reward}\")\n",
    "            \n",
    "            if pickup_change_config and terminal_count == pickup_change_config[0]: # This would only be relevant for Experiment 4\n",
    "                world.pickup_locs = pickup_change_config[1]\n",
    "                print(f\"Pickup locations changed to: {world.pickup_locs}\")\n",
    "            \n",
    "            world.reset()\n",
    "            episode_steps = 0\n",
    "            episode_reward = 0\n",
    "        \n",
    "        if policy_idx + 1 < len(policy_schedule) and current_step >= policy_schedule[policy_idx + 1][0]: # Change policy if needed, like after 500 steps\n",
    "            policy_idx += 1\n",
    "            current_policy = policy_schedule[policy_idx][1]\n",
    "            print(f\"\\nSwitching to policy {current_policy} at step {current_step}\")\n",
    "        \n",
    "        state_f = world.get_state('F')\n",
    "        valid_actions_f = world.get_valid_actions('F')\n",
    "        \n",
    "        if valid_actions_f:\n",
    "            action_f = agent_f.choose_action(state_f, valid_actions_f, current_policy)\n",
    "            reward_f = world.execute_action('F', action_f)\n",
    "            next_state_f = world.get_state('F')\n",
    "            next_valid_actions_f = world.get_valid_actions('F')\n",
    "            next_action_f = agent_f.choose_action(next_state_f, next_valid_actions_f, current_policy) if next_valid_actions_f else None\n",
    "            \n",
    "            agent_f.update_q_value(state_f, action_f, reward_f, next_state_f, next_action_f, next_valid_actions_f)\n",
    "            \n",
    "            episode_reward += reward_f\n",
    "            rewards_history.append(reward_f)\n",
    "        \n",
    "        current_step += 1\n",
    "        episode_steps += 1\n",
    "        \n",
    "        if current_step >= total_steps:\n",
    "            break\n",
    "        \n",
    "        state_m = world.get_state('M')\n",
    "        valid_actions_m = world.get_valid_actions('M')\n",
    "        \n",
    "        if valid_actions_m: # Female goes first, then male\n",
    "            action_m = agent_m.choose_action(state_m, valid_actions_m, current_policy)\n",
    "            reward_m = world.execute_action('M', action_m)\n",
    "            next_state_m = world.get_state('M')\n",
    "            next_valid_actions_m = world.get_valid_actions('M')\n",
    "            next_action_m = agent_m.choose_action(next_state_m, next_valid_actions_m, current_policy) if next_valid_actions_m else None\n",
    "            \n",
    "            agent_m.update_q_value(state_m, action_m, reward_m, next_state_m, next_action_m, next_valid_actions_m)\n",
    "            \n",
    "            episode_reward += reward_m\n",
    "            rewards_history.append(reward_m)\n",
    "        \n",
    "        current_step += 1\n",
    "        episode_steps += 1\n",
    "        \n",
    "        if current_step % 1000 == 0:\n",
    "            print(f\"Step {current_step}/{total_steps} - Terminals: {terminal_count}\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(f\"\\nExperiment completed:\")\n",
    "    print(f\"Total terminal states reached: {terminal_count}\")\n",
    "    print(f\"Average steps per episode: {np.mean(steps_per_episode) if steps_per_episode else 0:.2f}\")\n",
    "    print(f\"Average reward per episode: {np.mean(total_rewards_per_episode) if total_rewards_per_episode else 0:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards_history': rewards_history,\n",
    "        'steps_per_episode': steps_per_episode,\n",
    "        'total_rewards_per_episode': total_rewards_per_episode,\n",
    "        'terminal_count': terminal_count,\n",
    "        'agent_f_qtable': dict(agent_f.q_table),\n",
    "        'agent_m_qtable': dict(agent_m.q_table)\n",
    "    }\n",
    "\n",
    "def plot_results(results, title): # This function plots the results of our experiments, some more visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    cumulative_rewards = np.cumsum(results['rewards_history'])\n",
    "    axes[0, 0].plot(cumulative_rewards)\n",
    "    axes[0, 0].set_xlabel('Step')\n",
    "    axes[0, 0].set_ylabel('Cumulative Reward')\n",
    "    axes[0, 0].set_title('Cumulative Rewards Over Time')\n",
    "    \n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    if results['total_rewards_per_episode']:\n",
    "        \n",
    "        axes[0, 1].plot(results['total_rewards_per_episode'])\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Total Reward')\n",
    "        axes[0, 1].set_title('Total Rewards Per Episode')\n",
    "        \n",
    "        axes[0, 1].grid(True)\n",
    "    \n",
    "    if results['steps_per_episode']:\n",
    "        axes[1, 0].plot(results['steps_per_episode'])\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Steps')\n",
    "        axes[1, 0].set_title('Steps Per Episode')\n",
    "        \n",
    "        axes[1, 0].grid(True)\n",
    "    \n",
    "    window = 100\n",
    "    if len(results['rewards_history']) >= window:\n",
    "        moving_avg = np.convolve(results['rewards_history'], np.ones(window)/window, mode='valid')\n",
    "        \n",
    "        axes[1, 1].plot(moving_avg)\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Average Reward')\n",
    "        axes[1, 1].set_title(f'Moving Average Reward (window={window})')\n",
    "        axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def print_qtable_sample(qtable, agent_name, num_states=10):\n",
    "    print(f\"{agent_name} Q-Table Sample (first {num_states} states):\")\n",
    "    \n",
    "    for i, (state, actions) in enumerate(list(qtable.items())[:num_states]):\n",
    "        print(f\"\\nState {i+1}: {state}\")\n",
    "        for action, q_value in actions.items():\n",
    "            print(f\"  Action {action}: {q_value:.4f}\")\n",
    "\n",
    "def calculate_manhattan_distance(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 1a: PRANDOM for all 8000 steps\")\n",
    "\n",
    "world_1a = PDWorld()\n",
    "agent_f_1a = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_1a = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "\n",
    "policy_schedule_1a = [(0, 'PRANDOM')]\n",
    "results_1a = run_experiment(world_1a, agent_f_1a, agent_m_1a, 8000, policy_schedule_1a, \"Experiment 1a\", seed=42)\n",
    "plot_results(results_1a, \"Experiment 1a: PRANDOM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 1b: PRANDOM (500 steps) then PGREEDY (7500 steps)\")\n",
    "\n",
    "\n",
    "world_1b = PDWorld()\n",
    "agent_f_1b = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_1b = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "\n",
    "policy_schedule_1b = [(0, 'PRANDOM'), (500, 'PGREEDY')]\n",
    "results_1b = run_experiment(world_1b, agent_f_1b, agent_m_1b, 8000, policy_schedule_1b, \"Experiment 1b\", seed=42)\n",
    "plot_results(results_1b, \"Experiment 1b: PRANDOM then PGREEDY after 500 steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 1c: PRANDOM (500 steps) then PEXPLOIT (7500 steps)\")\n",
    "\n",
    "world_1c = PDWorld()\n",
    "agent_f_1c = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_1c = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "\n",
    "policy_schedule_1c = [(0, 'PRANDOM'), (500, 'PEXPLOIT')]\n",
    "results_1c = run_experiment(world_1c, agent_f_1c, agent_m_1c, 8000, policy_schedule_1c, \"Experiment 1c\", seed=42)\n",
    "\n",
    "plot_results(results_1c, \"Experiment 1c: PRANDOM then PEXPLOIT after 500 steps\")\n",
    "\n",
    "print_qtable_sample(results_1c['agent_f_qtable'], \"Agent F\", num_states=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 2: SARSA with PRANDOM (500 steps) then PEXPLOIT (7500 steps)\")\n",
    "\n",
    "world_2 = PDWorld()\n",
    "agent_f_2 = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='SARSA')\n",
    "agent_m_2 = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='SARSA')\n",
    "\n",
    "policy_schedule_2 = [(0, 'PRANDOM'), (500, 'PEXPLOIT')]\n",
    "results_2 = run_experiment(world_2, agent_f_2, agent_m_2, 8000, policy_schedule_2, \"Experiment 2 (SARSA)\", seed=42)\n",
    "plot_results(results_2, \"Experiment 2: SARSA with PRANDOM then PEXPLOIT after 500 steps\")\n",
    "\n",
    "print_qtable_sample(results_2['agent_f_qtable'], \"Agent F\", num_states=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 3a: Q-learning with alpha=0.15\")\n",
    "\n",
    "world_3a = PDWorld()\n",
    "agent_f_3a = QLearningAgent('F', alpha=0.15, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_3a = QLearningAgent('M', alpha=0.15, gamma=0.5, algorithm='Q-learning')\n",
    "\n",
    "policy_schedule_3 = [(0, 'PRANDOM'), (500, 'PEXPLOIT')]\n",
    "results_3a = run_experiment(world_3a, agent_f_3a, agent_m_3a, 8000, policy_schedule_3, \"Experiment 3a (alpha=0.15)\", seed=42)\n",
    "plot_results(results_3a, \"Experiment 3a: alpha=0.15\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Experiment 3b: Q-learning with alpha=0.45\")\n",
    "\n",
    "world_3b = PDWorld()\n",
    "agent_f_3b = QLearningAgent('F', alpha=0.45, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_3b = QLearningAgent('M', alpha=0.45, gamma=0.5, algorithm='Q-learning')\n",
    "\n",
    "results_3b = run_experiment(world_3b, agent_f_3b, agent_m_3b, 8000, policy_schedule_3, \"Experiment 3b (alpha=0.45)\", seed=42)\n",
    "plot_results(results_3b, \"Experiment 3b: alpha=0.45\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 4: Pickup locations change after 3rd terminal state\")\n",
    "\n",
    "world_4 = PDWorld()\n",
    "agent_f_4 = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_4 = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "\n",
    "policy_schedule_4 = [(0, 'PRANDOM'), (500, 'PEXPLOIT')]\n",
    "pickup_change = (3, [(1,2), (4,5)])\n",
    "results_4 = run_experiment(world_4, agent_f_4, agent_m_4, 8000, policy_schedule_4, \"Experiment 4\", pickup_change_config=pickup_change, seed=42)\n",
    "plot_results(results_4, \"Experiment 4: Adaptation to Pickup Location Changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparison of All Experiments\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "\n",
    "if results_1a['total_rewards_per_episode']:\n",
    "    ax1.plot(results_1a['total_rewards_per_episode'], label='1a: PRANDOM', alpha=0.7)\n",
    "if results_1b['total_rewards_per_episode']:\n",
    "    ax1.plot(results_1b['total_rewards_per_episode'], label='1b: PGREEDY', alpha=0.7)\n",
    "if results_1c['total_rewards_per_episode']:\n",
    "    ax1.plot(results_1c['total_rewards_per_episode'], label='1c: PEXPLOIT', alpha=0.7)\n",
    "    \n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Experiment 1: Policy Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "\n",
    "if results_1c['total_rewards_per_episode']:\n",
    "    ax2.plot(results_1c['total_rewards_per_episode'], label='Q-learning', alpha=0.7)\n",
    "if results_2['total_rewards_per_episode']:\n",
    "    ax2.plot(results_2['total_rewards_per_episode'], label='SARSA', alpha=0.7)\n",
    "    \n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Total Reward')\n",
    "ax2.set_title('Experiment 2: Q-learning vs SARSA')\n",
    "\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "if results_1c['total_rewards_per_episode']:\n",
    "    ax3.plot(results_1c['total_rewards_per_episode'], label='alpha=0.30', alpha=0.7)\n",
    "if results_3a['total_rewards_per_episode']:\n",
    "    ax3.plot(results_3a['total_rewards_per_episode'], label='alpha=0.15', alpha=0.7)\n",
    "if results_3b['total_rewards_per_episode']:\n",
    "    ax3.plot(results_3b['total_rewards_per_episode'], label='alpha=0.45', alpha=0.7)\n",
    "    \n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Total Reward')\n",
    "ax3.set_title('Experiment 3: Learning Rate Comparison')\n",
    "ax3.legend()\n",
    "\n",
    "ax3.grid(True)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "if results_4['total_rewards_per_episode']:\n",
    "    ax4.plot(results_4['total_rewards_per_episode'], label='Exp 4: Adaptation', alpha=0.7, marker='o')\n",
    "    ax4.axvline(x=3, color='r', linestyle='--', label='Pickup locations changed')\n",
    "    \n",
    "ax4.set_xlabel('Episode')\n",
    "ax4.set_ylabel('Total Reward')\n",
    "ax4.set_title('Experiment 4: Adaptation to Environment Changes')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary Statistics\")\n",
    "\n",
    "experiments = {\n",
    "    '1a (PRANDOM)': results_1a,\n",
    "    '1b (PGREEDY)': results_1b,\n",
    "    '1c (PEXPLOIT)': results_1c,\n",
    "    '2 (SARSA)': results_2,\n",
    "    '3a (alpha=0.15)': results_3a,\n",
    "    '3b (alpha=0.45)': results_3b,\n",
    "    '4 (Adaptation)': results_4\n",
    "}\n",
    "\n",
    "for name, results in experiments.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Terminal states reached: {results['terminal_count']}\")\n",
    "    if results['steps_per_episode']:\n",
    "        print(f\"  Avg steps per episode: {np.mean(results['steps_per_episode']):.2f}\")\n",
    "        print(f\"  Avg reward per episode: {np.mean(results['total_rewards_per_episode']):.2f}\")\n",
    "    print(f\"  Total cumulative reward: {sum(results['rewards_history']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run all experiments again with different random seed for comparison\n",
    "print(\"Running experiments again with different random seed for comparison\")\n",
    "\n",
    "world_1c_r2 = PDWorld()\n",
    "agent_f_1c_r2 = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_1c_r2 = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "results_1c_r2 = run_experiment(world_1c_r2, agent_f_1c_r2, agent_m_1c_r2, 8000, policy_schedule_1c, \"Experiment 1c Run 2\", seed=123)\n",
    "\n",
    "world_2_r2 = PDWorld()\n",
    "agent_f_2_r2 = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='SARSA')\n",
    "agent_m_2_r2 = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='SARSA')\n",
    "results_2_r2 = run_experiment(world_2_r2, agent_f_2_r2, agent_m_2_r2, 8000, policy_schedule_2, \"Experiment 2 Run 2\", seed=123)\n",
    "\n",
    "world_3a_r2 = PDWorld()\n",
    "agent_f_3a_r2 = QLearningAgent('F', alpha=0.15, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_3a_r2 = QLearningAgent('M', alpha=0.15, gamma=0.5, algorithm='Q-learning')\n",
    "results_3a_r2 = run_experiment(world_3a_r2, agent_f_3a_r2, agent_m_3a_r2, 8000, policy_schedule_3, \"Experiment 3a Run 2\", seed=123)\n",
    "\n",
    "world_4_r2 = PDWorld()\n",
    "agent_f_4_r2 = QLearningAgent('F', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "agent_m_4_r2 = QLearningAgent('M', alpha=0.3, gamma=0.5, algorithm='Q-learning')\n",
    "results_4_r2 = run_experiment(world_4_r2, agent_f_4_r2, agent_m_4_r2, 8000, policy_schedule_4, \"Experiment 4 Run 2\", pickup_change_config=pickup_change, seed=123)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Compare: Run 1 vs Run 2\")\n",
    "\n",
    "comparisons = [\n",
    "    (\"1c (PEXPLOIT)\", results_1c, results_1c_r2),\n",
    "    (\"2 (SARSA)\", results_2, results_2_r2),\n",
    "    (\"3a (alpha=0.15)\", results_3a, results_3a_r2),\n",
    "    (\"4 (Adaptation)\", results_4, results_4_r2)\n",
    "]\n",
    "\n",
    "for name, run1, run2 in comparisons:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Run 1 - Terminals: {run1['terminal_count']}, Avg reward: {np.mean(run1['total_rewards_per_episode']) if run1['total_rewards_per_episode'] else 0:.2f}\")\n",
    "    print(f\"  Run 2 - Terminals: {run2['terminal_count']}, Avg reward: {np.mean(run2['total_rewards_per_episode']) if run2['total_rewards_per_episode'] else 0:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
